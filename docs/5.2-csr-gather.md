# Blueprint 2: CSR gather with a sharded work list

Goal: **exact per‑edge attraction on WebGL2** with **deterministic writes**, **no atomics**, and **no reliance on float blending** (though we’ll also show how to use blending if available).

---

## 0) Why Blueprint 2?

- **Deterministic**: each node is written once per pass (no non‑associative blending sums).
- **Portable**: works on plain WebGL2 + `EXT_color_buffer_float` (for 16F/32F render targets). No need for `EXT_float_blend`, though we can use it to speed up the reduction.
- **Load‑balanced**: sharding keeps per‑fragment loop lengths bounded (critical for hubs with thousands of neighbors).
- **Compositional**: drop the result into your existing KDK integrator alongside Barnes–Hut gravity.

---

## 1) Data model & GPU textures

We represent the graph in **CSR** and pre‑split each node’s adjacency into fixed‑size **shards**.

### 1.1 Core arrays (on CPU first, then upload)

- **CSR**

  - `rowPtr` (length `N+1`) — prefix offsets into `colIdx`.
  - `colIdx` (length `E`) — neighbor indices.
  - `weight` (length `E`) — per‑edge weight `w_ij` (optional).

- **Degrees**

  - `deg` (length `N`) — degree or weighted degree.
    Useful for normalized Laplacian (dissuade hubs) and to compute `D x`.

- **Shards (work list)**

  - Pick a **shard size** `L` (e.g., 64). For each node `i` with degree `d`, emit

    ```
    for (int off = 0; off < d; off += L)
      shards.push({ nodeId: i, start: rowPtr[i] + off, len: min(L, d - off) });
    ```

  - Keep `nodeShardPtr` (length `N+1`): prefix‑sum of **#shards per node**. This ensures all shards for a node are **contiguous**—handy for segmented reduction.

> **Encoding ints as floats:** In WebGL2 you can store indices as 32‑bit floats (exact for integers up to ~16,777,216). If you may exceed that, split indices into two 16‑bit halves across RG channels and reconstruct in the shader.

### 1.2 GPU texture choices

- Positions: `RGBA32F` (x,y,z,extra) — you already have this.
- CSR:

  - `rowPtr`: `R32F` (N+1).
  - `colIdx`: `R32F` (E) **or** `RG16F/RG32F` if using hi/lo packing.
  - `weight`: `R16F` (E) is usually enough; use `R32F` for wide dynamic range.

- Degrees: `R32F` (N) and optionally `degInv` in `R32F` if you normalize edges.
- Shards: `RGBA32F` (S) with `(nodeId, start, len, 0)`; `S ≈ ceil(E/L)`.

> Texture sizing: for a linear array of length `M`, pick `w = min(maxTexSize, ceil(sqrt(M)))`, `h = ceil(M / w)`. Always set viewport to the FBO dimensions for each pass.

---

## 2) Per‑frame pipeline

We compute (A\mathbf{x}) (neighbor sum) via **two passes**, then finish (-L\mathbf{x} = (A\mathbf{x}) - (D\mathbf{x})) per node.

### Pass 1 — **Shard partials** (CSR gather)

- **One fragment per shard**.
- Each fragment loops over up to `L` neighbors, fetching the neighbor’s position and edge weight, and accumulates ( \sum w\_{ij}, \mathbf{x}\_j ) into registers.
- Output per shard: **partial sum** (p*s = \sum w*{ij}, \mathbf{x}_j) and optionally **sum of weights** (w_s = \sum w_{ij}) (useful if you want `Ax` and also the diagonal term `Dx` in a single pass).

**FS (fragment shader) for partials (GLSL 300 es):**

```glsl
#version 300 es
precision highp float; precision highp int;

uniform sampler2D uPos;       // RGBA32F, positions
uniform sampler2D uColIdx;    // R32F, neighbor indices
uniform sampler2D uWeight;    // R16F/R32F, weights
uniform sampler2D uShards;    // RGBA32F: (nodeId, start, len, 0)
uniform ivec2 posSize, colSize, wgtSize, shardSize;
uniform int L;                // shard capacity

layout(location=0) out vec4 outPartial;  // xyz: sum w*xj; w: sum w

// 1D linear index -> 2D coords:
ivec2 toUV(int idx, ivec2 size) { return ivec2(idx % size.x, idx / size.x); }
float fFetch1D(sampler2D t, ivec2 size, int idx) {
  return texelFetch(t, toUV(idx, size), 0).x;
}
vec4 vFetch1D(sampler2D t, ivec2 size, int idx) {
  return texelFetch(t, toUV(idx, size), 0);
}

void main() {
  // Current shard id from pixel coords:
  ivec2 uv = ivec2(int(gl_FragCoord.x) - 0, int(gl_FragCoord.y) - 0);
  int sid = uv.y * shardSize.x + uv.x;
  vec4 srec = vFetch1D(uShards, shardSize, sid);
  int start = int(srec.y + 0.5);
  int len   = int(srec.z + 0.5);

  vec3 sumx = vec3(0.0);
  float wsum = 0.0;

  // Fixed upper bound loop; early-exit via 'if':
  for (int k = 0; k < 256; ++k) { // 256 >= max L you will use
    if (k >= len) break;
    int e   = start + k;
    int nbr = int(fetch1D(uColIdx, colSize, e) + 0.5);
    float w = fFetch1D(uWeight, wgtSize, e);
    vec3 xj = vFetch1D(uPos, posSize, nbr).xyz;
    sumx += w * xj;
    wsum += w;
  }
  outPartial = vec4(sumx, wsum);
}
```

> **Note:** If you prefer, move the loop to the **vertex shader** and draw points; with `gl.POINTS` it’s equivalent. Fragment is fine because we need one output per shard (one pixel).

### Pass 2 — **Reduction per node** (partials → `Ax`)

**Two ways:**

**2A. With float blending (fast & simple):**

- Bind FBO with target texture `AxTex` (size `N`).
- Set `gl.blendFunc(ONE, ONE)`.
- Draw **one point per shard**; in the VS place each point at pixel for its `nodeId`; in the FS output the shard partial.
- Hardware additively blends all partials into their node’s pixel.
- Optional: also blend **(-D\mathbf{x})** here by subtracting `deg[i]*xi` from `Ax`. (Or do it in a tiny per‑node pass.)

**2B. Without blending (portable & deterministic):**

- Use a **segmented reduction** because shards are stored **contiguously per node**:

  - Maintain `nodeShardPtr` (N+1) so you know each node’s segment `[start,end)`.
  - Do a **binary‑tree reduction over each segment** in (\lceil \log_2(\text{maxShardsPerNode}) \rceil) passes.
    Pass `t` sums pairs at distance `stride = 2^t`. If a pair straddles segment boundaries, leave value unchanged.
  - After `T` passes, read **the first element** of each segment as the node’s `Ax`.

**FS for one reduction pass (ping‑pong between two partial textures):**

```glsl
#version 300 es
precision highp float; precision highp int;

uniform sampler2D uPartials;     // current partials (S elements)
uniform sampler2D uShardToNode;  // R32F: nodeId per shard (S elements)
uniform sampler2D uNodeShardPtr; // R32F: (N+1) segment offsets
uniform int stride;              // 1,2,4,8,...

uniform ivec2 partSize, mapSize, segSize; // sizes

layout(location=0) out vec4 outPartial; // next partials

ivec2 toUV(int idx, ivec2 size) { return ivec2(idx % size.x, idx / size.x); }
float fFetch1D(sampler2D t, ivec2 size, int idx) {
  return texelFetch(t, toUV(idx, size), 0).x;
}
vec4 vFetch1D(sampler2D t, ivec2 size, int idx) {
  return texelFetch(t, toUV(idx, size), 0);
}

void main() {
  int sid = int(gl_FragCoord.y) * partSize.x + int(gl_FragCoord.x);
  // Current node segment:
  int node = int(fFetch1D(uShardToNode, mapSize, sid) + 0.5);
  int segStart = int(fFetch1D(uNodeShardPtr, segSize, node) + 0.5);
  int segEnd   = int(fFetch1D(uNodeShardPtr, segSize, node + 1) + 0.5);

  // Try to sum sid with sid+stride if both in same segment:
  int mate = sid + stride;
  vec4 a = vFetch1D(uPartials, partSize, sid);
  if (mate < segEnd && // mate is inside same segment
      ( (sid - segStart) % (2*stride) == 0 ) ) {
    vec4 b = vFetch1D(uPartials, partSize, mate);
    outPartial = a + b;
  } else {
    outPartial = a;
  }
}
```

After (\lceil \log_2(\text{maxSegLen})\rceil) passes, **the head of each segment** (index `segStart`) holds the full sum for that node. Run a tiny pass that reads those `segStart` entries and writes them compactly into `AxTex` (size `N`).

**Compact‑write pass (one pixel per node):**

```glsl
#version 300 es
precision highp float;
uniform sampler2D uPartials;     // reduced partials
uniform sampler2D uNodeShardPtr; // segment starts
uniform ivec2 partSize, segSize, nodeTexSize;

layout(location=0) out vec4 outAx;

ivec2 toUV(int idx, ivec2 size) { return ivec2(idx % size.x, idx / size.x); }
vec4 vFetch1D(sampler2D t, ivec2 size, int idx){
  return texelFetch(t, toUV(idx, size), 0);
}

void main() {
  ivec2 uv = ivec2(int(gl_FragCoord.x), int(gl_FragCoord.y));
  int node = uv.y * nodeTexSize.x + uv.x;
  int segStart = int(texelFetch(uNodeShardPtr, toUV(node, segSize), 0).x + 0.5);
  outAx = texelFetch(uPartials, toUV(segStart, partSize), 0);
}
```

### Pass 3 — **Apply Laplacian and combine**

For each node `i`:

- Fetch `Ax[i]`, `deg[i]`, `x[i]`.
- Compute **attraction force**:
  `F_attr = k * (Ax[i] - deg[i] * x[i])`.
  (For **normalized** Laplacian, pre‑scale edges by `1/deg[src]` or use `degInv` textures and adjust.)
- Add BH repulsion + other forces, clamp by `maxAccel`, and integrate.

This pass is a single, simple per‑node shader.

---

## 3) API hooks & wiring into THREE‑g

Minimal new state you’ll want to expose:

```ts
type GraphForceConfig = {
  csr: {
    rowPtrTex: WebGLTexture; // R32F (N+1)
    colIdxTex: WebGLTexture; // R32F or RG16F packed (E)
    weightTex: WebGLTexture; // R16F or R32F (E)
    degTex: WebGLTexture; // R32F (N)
  };
  shards: {
    recordsTex: WebGLTexture; // RGBA32F (S): nodeId, start, len, pad
    nodeShardPtrTex: WebGLTexture; // R32F (N+1)
    shardToNodeTex: WebGLTexture; // R32F (S) – optional if not derivable
    L: number;
  };
  k: number; // spring constant
  normalized: boolean; // FA2 “dissuade hubs”
};
```

In your **frame loop**:

1. `bh.compute()` → repulsion acceleration in `accBH`.
2. **Graph (Blueprint 2)**:

   - Pass 1: shards → partials (texture `partialsA`).
   - Pass 2: segmented reduce `partialsA` → `partialsB` (ping‑pong per level).
   - Compact write: `partials*` → `AxTex`.
   - Per‑node Laplacian finish: `AxTex`, `degTex`, `posTex` → `accGraph`.

3. **Combine**: sum `accBH + accGraph + gravity` (one pass).
4. **Integrate**: your existing KDK step.

---

## 4) Choosing parameters & scheduling

- **Shard size `L`**:

  - Desktop GPUs: 64 or 96 is a good start.
  - Integrated/mobile GPUs: 32 or 48.
    Larger `L` → fewer shards, fewer reduction passes, but longer per‑fragment loops. Smaller `L` → more shards but better load balance and cache reuse.

- **Batching**: If your `S` or `E` is huge, process shards in **tiles** (e.g., 1–5M shards per batch). You’ll accumulate `Ax` across batches (either via blending or by summing intermediate `Ax` textures).

- **Reordering for locality** (strongly recommended):

  - Periodically reindex nodes by a **space‑filling curve** (Hilbert/Morton) of current `(x,y[,z])` to improve texture locality for `uPos` fetches.
  - Rebuild CSR and shards with the new IDs. (This can be done every few seconds of sim time and will noticeably raise throughput.)

- **Dynamic graphs**: If edges change frequently, amortize CSR + shard rebuilds (e.g., rebuild every K frames or when the change set exceeds a threshold). Keep 2 CSR/shard sets and swap to hide stalls.

---

## 5) Memory & complexity

- **Compute complexity**:

  - Pass 1: (O(E)) operations, memory‑bound.
  - Pass 2: (O(S \log M)) (where (M) is max shards per node; usually small).
  - Compact write + Laplacian finish: (O(N)).

- **Memory (approx, float‑encoded indices):**

  - `colIdx`: `4E` bytes (or `4E` packed half‑floats split across RG if needed).
  - `weight`: `2E`–`4E` bytes.
  - `rowPtr`: `4(N+1)` bytes.
  - `deg`: `4N` bytes.
  - `shards`: `16S` bytes (`RGBA32F`), with `S≈E/L`.
  - `partials`: `8S` bytes if `RGBA16F`, `16S` if `RGBA32F`.

For example, **N = 2e5, E = 4e6, L = 64** → `S ≈ 62.5k`:

- CSR ~ (colIdx 16 MB + weight 8–16 MB + rowPtr ~0.8 MB) ≈ **25–33 MB**
- Shards ~ 1 MB; Partials (16F) ~ 0.5 MB
  Total new memory well under **40 MB** plus your existing sim textures.

---

## 6) Optional accelerations & variants

- **SELL‑C‑σ / SELL‑P**: Instead of shards, store adjacency in **sliced ELLPACK** blocks (rows sorted by length in small tiles). That gives fixed‑length loops and often better cache behavior when the graph is _static_. It’s a sideways move from CSR+shards with similar performance characteristics.

- **With float‑blending available**:
  You can skip segmented reduction entirely:

  1. Pass 1: output partials to a “scatter buffer” (one point per shard) positioned at `nodeId` pixel with **additive blending** into `AxTex`.
  2. (Optional) Combine (-D\mathbf{x}) here by outputting `Ax - deg*x` directly.
     This is extremely simple and generally faster on desktop GPUs, but not deterministic.

- **Normalized Laplacian (FA2’s “dissuade hubs”)**:
  Multiply each edge weight by `1/deg[src]` up front (when building CSR) or fetch `degInv[src]` in the partials shader and scale `w` per neighbor.

- **Mini‑batch edges**: For very high `E`, process a random subset of shards each frame and scale forces by `1/p` (unbiased). With mild damping and KDK, this converges well for layouts.

---

## 7) Drop‑in GL state snippets (what to actually set)

**Pass 1 FBO setup**

```js
gl.bindFramebuffer(gl.FRAMEBUFFER, fboPartials);
gl.framebufferTexture2D(
  gl.FRAMEBUFFER,
  gl.COLOR_ATTACHMENT0,
  gl.TEXTURE_2D,
  partialsTexA,
  0
);
gl.viewport(0, 0, shardsW, shardsH);
gl.disable(gl.BLEND);
// bind uPos, uColIdx, uWeight, uShards, set uniforms (sizes, L)
drawFullScreenQuad(); // or draw points S-sized
```

**Pass 2 segmented reduce (k rounds)**

```js
for (let stride = 1; stride < maxSegLen; stride <<= 1) {
  gl.bindFramebuffer(gl.FRAMEBUFFER, fboPartials);
  gl.framebufferTexture2D(
    gl.FRAMEBUFFER,
    gl.COLOR_ATTACHMENT0,
    gl.TEXTURE_2D,
    ping ? partialsTexB : partialsTexA,
    0
  );
  gl.viewport(0, 0, shardsW, shardsH);
  // bind uPartials = ping? partialsTexA : partialsTexB
  // bind uShardToNode, uNodeShardPtr, set uniform 'stride'
  drawFullScreenQuad();
  ping = !ping;
}
```

**Compact write to `AxTex`**

```js
gl.bindFramebuffer(gl.FRAMEBUFFER, fboAx);
gl.framebufferTexture2D(
  gl.FRAMEBUFFER,
  gl.COLOR_ATTACHMENT0,
  gl.TEXTURE_2D,
  AxTex,
  0
);
gl.viewport(0, 0, nodesW, nodesH);
// bind reduced uPartials = (ping? partialsTexA : partialsTexB)
// bind uNodeShardPtr
drawFullScreenQuad();
```

**Laplacian finish**

```js
gl.bindFramebuffer(gl.FRAMEBUFFER, fboAccGraph);
gl.framebufferTexture2D(
  gl.FRAMEBUFFER,
  gl.COLOR_ATTACHMENT0,
  gl.TEXTURE_2D,
  accGraphTex,
  0
);
gl.viewport(0, 0, nodesW, nodesH);
// bind uAxTex, uPosTex, uDegTex, k
drawFullScreenQuad();
```

---

## 8) Numerical & integration notes

- Use your **KDK** integrator. Keep a global **speed/cooling** scalar (FA2 “jitter tolerance” analogue).
- Clamp **maxAccel**; when mini‑batching shards, this prevents hub spikes.
- With segmented reduction you get deterministic sums—useful for testing and cross‑platform stability. Switch to blending when you need more raw throughput and can tolerate small nondeterminism.

---

## 9) Testing & debugging

- **CPU parity check** on tiny graphs (≤1k edges): compute `Ax` on CPU and compare to GPU within a small tolerance.
- **Heatmap diagnostics**: render `#shards per node` to a texture to see hotspots; pick `L` accordingly.
- **Timer queries**: if you already support GPU profiling, instrument each pass to tune `L`, tiling, and reordering cadence.

---

## 10) When to prefer Blueprint 2 vs. others

- Prefer **Blueprint 2** when you need **portability** (no float blending), **determinism**, or **very high‑degree hubs** that would stress scatter blending.
- Prefer **Blueprint 1 (scatter + blending)** if your target platforms have `EXT_float_blend` and you want the simplest, usually fastest path.
- Prefer **Blueprint 3 (pure SpMV)** when the attraction is strictly **linear**; then Blueprint 2’s implementation _is_ your SpMV (this blueprint _implements_ the “gather” half of Blueprint 3). For LinLog or nonlinear edge forces, add a small SDDMM edge pass before the same aggregation pipeline.

---

If you’d like, I can tailor this into a ready‑to‑drop **`graphAttractionPass()`** module for THREE‑g (textures in, textures out), including a compact **texture layout helper** that chooses dimensions, creates FBOs, and runs Pass 1/2/3 with a single call.
