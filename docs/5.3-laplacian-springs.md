# RAPIDS cuGraph and Laplacian springs

1. **What is worth borrowing from RAPIDS cuGraph ForceAtlas2 (FA2)**—even if you don’t have CUDA/WebGPU features.
2. **A deep dive on Blueprint 3 (Laplacian springs = SpMV)**—how to make it practical and fast in WebGL2 for your use case.

---

## 1) What to borrow from RAPIDS cuGraph ForceAtlas2

cuGraph’s FA2 sticks fairly close to the Gephi/ForceAtlas2 paper but hardens it for scale. Even if you can’t use CUDA atomics or shared memory in WebGL2, you can **port the ideas and scheduling**.

### Algorithmic features to adopt (portable)

- **Barnes–Hut for repulsion**: You already have this. Keep using BH for all‑to‑all repulsion and tune `theta` exactly like FA2. cuGraph exposes `barnes_hut_optimize` and `barnes_hut_theta`, matching the paper. ([RAPIDS Docs][1])

- **Attraction on edges (exact)**: FA2 does _exact_ per‑edge attraction (O(E)) and pairs it with BH repulsion. That’s precisely what your new “pair-wise forces” stage is. In FA2 the default attraction is _linear in distance_ (Hooke-like without rest length), and in **LinLog** mode it becomes `∝ log(1 + d)`. You can mirror both: linear attraction fits Blueprint 3 (SpMV), LinLog requires an edge pass (SDDMM→SpMM). ([PLOS][2])

- **Dissuade hubs / Outbound attraction distribution**: FA2 reduces hub dominance by **normalizing attraction by the source node degree**. In Laplacian terms, this is a **normalized adjacency**: use (D^{-1}A) (or (D^{-1/2} A D^{-1/2})) instead of (A). Implement this by precomputing `deg` and scaling edge weights by `1/deg[src]`. cuGraph exposes it as `outbound_attraction_distribution`. ([RAPIDS Docs][1])

- **Edge weight influence**: Just multiply per‑edge forces by `w^p` (default (p=1)). This is standard in FA2 (`edge_weight_influence`). ([RAPIDS Docs][1])

- **Global gravity / strong gravity**: FA2 pulls nodes to the origin with a tunable gravity and a “strong” variant. On the GPU this is a trivial per‑node add: ( \mathbf{F}\_i \mathrel{+}= -g,\mathbf{x}\_i) (or a stronger non‑linear variant). cuGraph exposes `gravity` and `strong_gravity_mode`. ([RAPIDS Docs][3])

- **Adaptive step sizing (“jitter tolerance” / speed)**: FA2 adjusts the simulation speed to damp oscillations and avoid exploding steps. Port this as a **global scalar “speed”** that adapts from the total displacement last frame (you can GPU‑reduce (\sum_i |\Delta x_i|) and adjust). cuGraph exposes `jitter_tolerance`. ([RAPIDS Docs][1])

- **Overlap prevention (optional)**: FA2 can push nodes apart to avoid overlap. In your case, just treat nodes as spheres with radius (r_i) and add a short‑range repulsion when (d<r_i+r_j). That can be folded into BH by inflating node radii or handled as a light extra pass. (FA2’s toggle: `prevent_overlapping`.) ([médialab Sciences Po][4])

- **Parameterization**: Keep FA2 names (theta, gravity, scaling_ratio, lin_log_mode, etc.). Users will instantly understand the knobs. ([RAPIDS Docs][1])

### System/engineering patterns to borrow (and adapt to WebGL2)

- **Separate O(N log N) repulsion from O(E) attraction**, and **stream edges in tiles**. cuGraph keeps the two loops distinct; you can do the same: run your BH pass, then process edges in 1–10M chunks to fit caches/driver limits. ([RAPIDS Docs][1])

- **Normalized / re‑weighted adjacency** to control hubs (above). This is crucial on social graphs.

- **Convergence heuristics**: compute total movement (a single scalar) and stop or cool when below threshold. (GPU reduction over displacements.) This mirrors FA2 “speed control.” ([PLOS][2])

- **Memory budgeting**: cuGraph docs & examples emphasize balancing node vs. edge memory. In WebGL2, store indices as 32‑bit floats (safe up to ~16 M distinct nodes) and weights in `R16F/R32F`. Keep an eye on VRAM: edge tables dominate. ([RAPIDS Docs][1])

> **Reality check:** FA2 in cuGraph relies on CUDA primitives (atomics, shared memory) that WebGL2 does not have. The good news: **you don’t need them.** The two WebGL‑friendly designs we discussed earlier—**additive blending (scatter)** and **CSR gather with sharded loops**—map well to FA2’s attraction stage and are proven at scale in browser engines like Cosmograph. ([GitHub][5])

---

## 2) Blueprint 3 in depth — **Laplacian springs = pure SpMV**

### 2.1 The model

If your per‑edge attraction is **linear in displacement**, i.e.
[
\mathbf{F}^{\text{attr}}*i ;=; k \sum*{j\in \mathcal{N}(i)} w_{ij},(\mathbf{x}_j - \mathbf{x}_i)
]
then
[
\mathbf{F}^{\text{attr}} ;=; k,(A\mathbf{x} - D\mathbf{x}) ;=; -k,L,\mathbf{x}
]
where (A) is (weighted) adjacency, (D) degree diag, and (L=D-A) the (combinatorial) graph Laplacian. That’s **three independent SpMVs**—one each for (x, y, z) (or two for 2D). If you enable FA2’s “dissuade hubs”, switch to a normalized Laplacian by premultiplying (A) with (D^{-1}) or (D^{-1/2}). This framing is exactly how high‑performance GPU graph frameworks (GraphBLAST, Gunrock, DGL) implement message passing. ([arXiv][6])

> **When Blueprint 3 applies:**
> ✓ Linear springs (FA2 default).
> ✗ LinLog attraction (needs per‑edge distance factor → do SDDMM⊕SpMM; see §2.6). ([PLOS][2])

---

### 2.2 A **WebGL2** implementation plan (portable & fast)

**Data (textures)**

- Positions: `RGBA32F` (x,y,z, extras).
- CSR adjacency:

  - `rowPtr` (N+1) in `R32F` (float‐encoded ints).
  - `colIdx` (E) in `R32F`.
  - `w` (E) in `R16F/R32F` (edge weights).

- Degrees `deg` (N) in `R32F` (and `deg^{-1}` if using normalized Laplacian).

> Storing indices as 32‑bit floats is common in WebGL; accurate up to ~16 M. For larger IDs, split across channels.

**Compute graph‑attraction via CSR “gather with shards”**

Variable‑length neighbor loops don’t vectorize well on GPUs and cause long shader loops. Use a **sharded work list**:

1. Offline (CPU once): for each node (i), split its neighbor list into chunks of size (L) (e.g., (L=64)): produce work items `(nodeId, start, len)`.
2. **Pass 1 (accumulate partials):** one fragment per work item; loop over its `len≤L` neighbors and sum ( \sum w\_{ij}, \mathbf{x}\_j). Output a **partial** to a `(numWorkItems)`‑sized texture.
3. **Pass 2 (reduce):** segmented‑reduce partials per node (either by additive blending into an `N`‑sized target, or with a small segmented‑sum pass). Add (-k,D\mathbf{x}) and you have (-k,L\mathbf{x}).
4. Combine with BH repulsion + gravity in your integration step.

This is the SpMV pattern (CSR gather + segmented reduction) used in GPU literature; the only WebGL‑specific twist is using textures & blending instead of CUDA atomics. ([mgarland.org][7])

**Shader sketch (Pass 1 partials)**

```glsl
// One fragment = one CSR shard (nodeId, start, len<=L)
#version 300 es
precision highp float; precision highp int;
uniform sampler2D uRowPtr, uColIdx, uWeight, uPos; // CSR + positions
uniform isampler2D uShards;     // packed (nodeId, start, len)
uniform int L;                  // shard block size
uniform ivec2 rowPtrSz, colIdxSz, posSz, shardSz;

out vec4 outPartial;  // xyz = sum_j w_ij * x_j ; wsum in .w (optional)

vec4 fetch1D(sampler2D tex, ivec2 sz, int idx){
  return texelFetch(tex, ivec2(idx % sz.x, idx / sz.x), 0);
}
ivec3 fetchShard(int sid){
  ivec2 uv = ivec2(sid % shardSz.x, sid / shardSz.x);
  ivec3 s  = texelFetch(uShards, uv, 0).xyz; // nodeId,start,len
  return s;
}

void main(){
  int sid = int(gl_FragCoord.y) * shardSz.x + int(gl_FragCoord.x);
  ivec3 sh = fetchShard(sid);
  int node = sh.x, start = sh.y, len = sh.z;

  vec3 sumx = vec3(0.0);
  float wsum = 0.0;
  for (int k=0; k<L; ++k){
    if (k >= len) break;
    int e    = start + k;
    int nbr  = int(fetch1D(uColIdx, colIdxSz, e).x + 0.5);
    float w  = fetch1D(uWeight, colIdxSz, e).x;
    // position of neighbor j:
    vec3 xj  = texelFetch(uPos, ivec2(nbr % posSz.x, nbr / posSz.x), 0).xyz;
    sumx += w * xj;
    wsum += w;
  }
  outPartial = vec4(sumx, wsum); // reduce later: Ax = sumx ; Dx = (deg * xi) in pass 2
}
```

**Pass 2 (reduce partials → node sums)**
Two options:

- **Additive blending** into an `N`‑sized target (one pixel per node) by rendering a point for each partial to its `nodeId` pixel; or
- A **segmented reduction** pass (pure functional; no blending). Both are standard. ([moderngpu.github.io][8])

Finally, compute (k(A\mathbf{x}) - k(D\mathbf{x})) per node and add BH repulsion + gravity in your existing integrator.

---

### 2.3 Throughput & viability: when is Blueprint 3 the win?

- **Complexity:** (O(E)) per iteration. On social graphs with heavy tails, (E) can be large—but the kernel is **memory‑bound and simple**, perfect for the GPU. With sharding, degree imbalance is tamed. This is exactly the workload SpMV kernels (GraphBLAST/merge‑based CSR) are designed to handle. ([mgarland.org][7])

- **Browser evidence:** Cosmograph/cosmos.gl demonstrates fully GPU, WebGL‑only force layouts handling **hundreds of thousands of nodes and links interactively**. They rely on the same ideas (shader passes, blending, MRT) and even call out `EXT_float_blend` as a key enabler. That validates the feasibility in WebGL2. ([GitHub][5])

- **When it shines most:**

  - You **want FA2 default (linear) attraction** and hub‑normalization → pure SpMV.
  - You need **determinism / extension‑free portability** → segmented reduction path avoids float blending.
  - You can **reindex nodes for locality** (see below) to boost texture‑cache hits.

- **Limits & budgets (rough guidance):**
  Keep **edge memory** the governing constraint: storing `colIdx`+`weight` as 8–12 bytes/edge (float‑encoded) → 5 M edges is ~40–60 MB plus overhead (textures, partials). That’s fine on desktop GPUs, pushing it on laptops/mobiles. Use **edge tiling** and **mini‑batches** when E is huge. (cuGraph notes similar practical concerns.) ([RAPIDS Docs][1])

---

### 2.4 Make it fast: layout & scheduling tricks that matter

- **Reorder nodes for locality (often!)**
  Periodically (every few seconds of sim time) renumber nodes by a **space‑filling curve** (Hilbert/Morton) based on current positions. Then rebuild CSR with the new IDs. This keeps neighbors close in memory so position fetches and CSR walks hit the texture cache. This trick consistently improves cache behavior in physics/mesh codes. ([ScienceDirect][9])

- **Degree sharding**
  The work‑list approach (fixed shard size (L)) prevents a handful of hubs from stalling your shader via long loops—standard in high‑performance SpMV/g‑SpMM. ([mgarland.org][7])

- **SELL‑C‑σ / SELL‑P (optional)**
  If you want _single‑pass_ gather without a work list, you can store adjacency in **sliced ELLPACK** blocks by **sorting rows by degree** in small tiles. That yields fixed‑length loops with far less padding than classic ELL. It’s a known win for GPU SpMV; it also plays well with texture fetches. (Great if your graph is static or changes rarely.) ([arXiv][10])

- **Mini‑batch edges per frame**
  For _very_ large E, process a random (p)‑fraction of edges each frame and scale by (1/p). With mild damping, KDK integration, and degree‑aware sampling, layouts converge well with far less per‑frame work.

---

### 2.5 Numeric stability

- Use your existing **KDK (Kick‑Drift‑Kick)** or velocity‑Verlet with a **global “speed”** (cooling) scalar.
- Clip **maxAccel** (you already expose it) to avoid hub slingshots when using mini‑batches.
- If you rely on **additive blending**, sums are order‑dependent; if that bothers you numerically, run the **segmented reduction** variant for deterministic accumulation. (That’s why I like the two‑pass design.) ([moderngpu.github.io][8])

---

### 2.6 When you _don’t_ want Blueprint 3

Two common FA2 features break pure SpMV:

- **LinLog mode**: the gradient of (\sum w*{ij}\log(1+d*{ij})) introduces a per‑edge factor (\propto 1/(1+d*{ij})). That’s **SDDMM (compute per‑edge scalar from positions) + SpMM (aggregate)**. In WebGL2: do **edge pass** (SDDMM) to compute (\alpha*{ij} = w*{ij}/(1+d*{ij})), then SpMM with (\alpha*{ij}) in place of (w*{ij}). (If you have `EXT_float_blend`, you can fuse by doing edge‑parallel scatter in one pass.) ([PLOS][2])

- **Edge‑specific nonlinearity** (e.g., rest‑length springs, saturations): same story—use an edge pass (SDDMM), then aggregate. For WebGPU/CUDA this is where **FusedMM** shines; in WebGL2 it’s typically 2 passes. ([arXiv][11])

---

## 3) Putting it all together in **THREE‑g**

**Recommended pipeline (WebGL2, portable & fast):**

1. **BH repulsion (you already have it)** → acceleration texture #1.
2. **Graph attraction (Blueprint 3)**

   - If **linear**: CSR gather + segmented reduction (or blending) → acceleration texture #2.
   - If **LinLog**: SDDMM edge pass (compute (\alpha\_{ij})) → CSR SpMM.
   - If **dissuade hubs**: scale weights by (1/\text{deg}[src]) (or use normalized (L)).

3. **Gravity / strong gravity** → per‑node add.
4. **Integrate (KDK), clip maxAccel, adaptive speed (FA2 “jitter tolerance”).**
5. **Every few seconds:** reindex nodes via Hilbert/Morton on current positions; rebuild CSR/SELL‑P; continue.

This gives you FA2‑grade behavior with your astrophysical gravity layered in, **entirely on WebGL2**, and matches how large systems do it: tree‑based O(N log N) for long‑range, plus O(E) edge forces expressed as sparse linear algebra/message passing. ([arXiv][6])

---

## 4) Short answers to your two questions

- **“What can we borrow from cuGraph FA2?”**
  The _ideas and knobs_: BH repulsion + exact edge attraction; dissuade hubs (degree normalization); LinLog option; gravity/strong gravity; adaptive speed via total displacement; overlap prevention; and a clean parameterization users recognize. All are implementable in WebGL2 with the scatter/gather patterns above—no CUDA atomics required. ([RAPIDS Docs][1])

- **“Is Blueprint 3 practical on WebGL2, and will it perform?”**
  Yes—when attraction is linear (the FA2 default), it maps perfectly to **SpMV**. The CSR‑gather‑with‑shards + segmented‑reduction pipeline is WebGL‑portable and _exactly_ the kind of memory‑bound work GPUs do well. For very large E, add sharding, reindexing for locality, and edge mini‑batches. Use SDDMM→SpMM for LinLog or other nonlinear edge forces. Cosmograph’s published results and the GPU SpMV literature back the approach. ([GitHub][5])

---

### References / resources

- cuGraph FA2 API & parameters (Barnes–Hut, gravity, dissuade hubs, jitter tolerance). ([RAPIDS Docs][1])
- ForceAtlas2 paper (Jacomy et al., 2014): linear vs LinLog attraction, parameters, speed control. ([PLOS][2])
- GraphBLAST & merge‑based CSR SpMV (load balancing for irregular degrees). ([arXiv][6])
- ModernGPU segmented reduction (reduce‑by‑key / CSR). ([moderngpu.github.io][8])
- FusedMM & GE‑SpMM (why SDDMM⊕SpMM matters for nonlinear edge forces). ([arXiv][11])
- Cosmograph (WebGL‑only, shader‑based force layout; uses float blending). ([GitHub][5])

If helpful, I can draft the concrete FBO bindings + GL state for (a) **CSR gather + segmented reduction** and (b) **edge‑parallel scatter with blending**, ready to drop into your `demo.js` loop.

[1]: https://docs.rapids.ai/api/cugraph/stable/api_docs/api/cugraph/cugraph.force_atlas2/?utm_source=chatgpt.com "cugraph.force_atlas2 — cugraph-docs 25.08.00 documentation"
[2]: https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0098679&utm_source=chatgpt.com "ForceAtlas2, a Continuous Graph Layout Algorithm for Handy ... - PLOS"
[3]: https://docs.rapids.ai/api/cugraph/stable/api_docs/cugraph_cpp/algorithms/layout_cpp/?utm_source=chatgpt.com "Layout — cugraph-docs 25.02.00 documentation - docs.rapids.ai"
[4]: https://medialab.sciencespo.fr/publications/Jacomy_Heymann_Venturini-Force_Atlas2.pdf?utm_source=chatgpt.com "ForceAtlas2, A Continuous Graph Layout Algorithm for Handy Network ..."
[5]: https://github.com/cosmosgl/graph/blob/main/README.md?utm_source=chatgpt.com "graph/README.md at main · cosmosgl/graph · GitHub"
[6]: https://arxiv.org/abs/1908.01407?utm_source=chatgpt.com "GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU"
[7]: https://mgarland.org/papers/2016/spmv/?utm_source=chatgpt.com "Michael Garland - Merge-based Parallel Sparse Matrix-vector Multiplication"
[8]: https://moderngpu.github.io/segreduce.html?utm_source=chatgpt.com "Segmented Reduction - Modern GPU - GitHub"
[9]: https://www.sciencedirect.com/science/article/abs/pii/S0021999123001043?utm_source=chatgpt.com "A cache-efficient reordering method for unstructured meshes with ..."
[10]: https://arxiv.org/abs/1307.6209?utm_source=chatgpt.com "A unified sparse matrix data format for efficient general sparse matrix-vector multiply on modern processors with wide SIMD units"
[11]: https://arxiv.org/pdf/2011.06391?utm_source=chatgpt.com "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph ..."
